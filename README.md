# This repository is the official implementation of [MST-UDA: A Unsupervised Domain Adaptation Framework via 2.5D Multi-Style Perceptual Translation Network and Self-Filtering for Cross-Modal Multi-Organ Segmentation](https://openreview.net/forum?id=fIeqn4x0yL&referrer=%5BAuthor+Console%5D%28%2Fgroup%3Fid%3DMICCAI.org%2F2025%2FChallenge%2FFLARE%2FAuthors%23your-submissions%29) of Team tju_vil_pioneers on FLARE24 challenge.

## üîç Overview
This work addresses the challenge of adapting CT-trained segmentation models to MRI and PET images, employing 2.5D Style Translation Method, Self-Filtering, and a two-stage segmentation framework in the FLARE25-Task3 dataset. For more details, see the pipeline diagram below:


## ‚öôÔ∏è Environments and Requirements
* Ubuntu 20.04.6 LTS
* Intel(R) Xeon(R) Platinum 8153 CPU @ 2.00GHz, RAM 192GB , NVIDIA Tesla V100 (32G)
* CUDA >= 11.3
* python >= 3.7.13


## üíæ Dataset
The training Data and validation data are provided by the [FLARE25](https://www.codabench.org/competitions/2296/#/pages-tab). In short, there are 50 fully labeled CT and 2000 unlabeled CT data, 4700+ unlabelled MRI and 1000 unlabelled PET data for training, 110 public MRI and 50 PET cases for validation. 

The CT scans are from the FLARE 2022 dataset where 50 cases have ground-truth labels and the remaining cases have pseudo labels (generated by the FLARE 2022 winning solution, ~90% DSC score). 

1.CT scans need to be manually adjusted, placing the 2000 pseudo-labeled and 300 manually labeled data into the corresponding folders under the CT directory.

2.The two MRI datasets 'AMOS_MR_good_spacing-833' and 'LLD-MMRI-3984' should both be placed under the MRI/Training directory.
3.The PET dataset should be placed under the PET/ directory.
```
|-- datasets
|   |-- CT
|   |   |-- CT2MRI_image
|   |   |-- CT2PET_image
|   |   |-- CT_image
|   |   `-- CT_label
|   |-- MRI
|   |   |-- PublicValidation
|   |   |   |-- imagesVal
|   |   |   `-- labelsVal
|   |   `-- Training
|   |       |-- AMOS_MR_good_spacing-833
|   |       |-- LLD-MMRI-3984
|   |-- PET
|   |   |-- image
|   |-- processed_data
|   |   |-- coarse
|   |   |   |-- combined_data
|   |   |   `-- small_segnet
|   |   `-- fine
|   |       |-- big_segnet
|   |       |-- combined_data
|   |       `-- small_segnet
|   `-- static_info
```

## ü™Ñ Preprocessing

Our preprocessing pipeline consists of three main steps:

### Step 1: CT and Synthetic MRI, PET Data Preprocessing
This step preprocesses the original CT data and fake MR data generated through style translation:
> Note1: Before preprocess, please refer to ["Style_Translation/README.md"](Style_Translation/README.md) for style translation detailed information.

> Note2: Before starting the preprocessing pipeline, please modify the data and file paths in `preprocess/preprocess_base.yaml`. You need to update:
- `MR_DATA_PREPROCESS.ROOT_PATH`: The absolute path to your datasets directory
- `DATASET.BASE_DIR`: The absolute path to your datasets directory 
- `FINE_MODEL_PATH` and `COARSE_MODEL_PATH`: The paths to your downloaded checkpoint directories

```bash
# Process original CT data
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step1_CT.yaml

# Process style translation fake MRI and PET data
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step1_FakeMRI.yaml
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step1_FakePET.yaml
```

### Step 2: MRI and PET Data Preprocessing and Pseudo-label Generation
This step handles two MR datasets (AMOS and LLD), including data filtering, registration, and pseudo-label generation:
```bash
# Process AMOS MRI dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step2_amos.yaml

# Process LLD MRI dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step2_lld.yaml

# Process PET dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step2_pet.yaml
```

### Step 3: Fine Segmentation Data Preprocessing
This step prepares data for the fine segmentation stage by adjusting patch size to [96,192,192]:
```bash
# Process AMOS MRI dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step3_amos_fine.yaml

# Process LLD MRI dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step3_lld_fine.yaml

# Process PET dataset
python ./preprocess/data_preprocess.py --cfg ./configs/preprocess/preprocess_step3_pet.yaml
```

## üñ•Ô∏è Train

Training our model consists of several main steps, as depicted in the pipeline figure above. Follow these instructions to conduct the training process effectively:

### Step 1: Domain Translation
Before training the segmentation models, we first translate the source domain images to target domain using CycleGAN.

- Train stage one image-to-image translation model for domain translation
```bash
python stage_1_i2i_train.py --name sourceAtotargetB
```
- Generate target-like source domain images
```bash
python stage_1.5_i2i_inference.py --ckpt_path YOUR_PATH --source_npy_dirpath SOURCE_PATH --target_npy_dirpath TARGET_PATH --save_npy_dirpath SAVE_PATH --k_means_clusters 6
```

### Step 2: Big SegNet Training

```bash
python train.py --cfg ./configs/train/train_big_segnet.yaml
```

### Step 3: Small SegNet Training

```bash
# coarse stage
python train.py --cfg train_small_segnet_coarse_stage.yaml
# fine stage
python train.py --cfg train_small_segnet_fine_stage.yaml
```


## üó≥Ô∏è Inference

To infer the testing cases, run this command:
An experiment of small segnet

```bash
python inference.py --cfg ./configs/inference/inference_small_segnet.yaml 
```


## üìã Results

Our method achieves the following performance on [FLARE25](https://www.codabench.org/competitions/2296/#/pages-tab)
MRI:
| Dataset Name       | DSC(%) | NSD(%) |
|--------------------|:------:|:------:|
| Validation Dataset | 80.77% | 87.22% |
| Test Dataset       | (?) | (?) |

PET:
| Dataset Name       | DSC(%) | NSD(%) |
|--------------------|:------:|:------:|
| Validation Dataset | 62.83% | 61.65% |
| Test Dataset       | (?) | (?) |


## üôè Acknowledgement

 We thank the contributors of [FLARE25 datasets](https://www.codabench.org/competitions/2296/#/pages-tab).
